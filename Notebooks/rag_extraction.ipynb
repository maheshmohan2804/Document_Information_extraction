{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Based Document Extraction Pipeline\n",
    "\n",
    "This notebook implements:\n",
    "1. Markdown-based chunking\n",
    "2. LLM tagging and metadata extraction\n",
    "3. Vector storage with ChromaDB\n",
    "4. Hybrid retrieval (Semantic + BM25)\n",
    "5. Final extraction using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites: pip install chromadb rank-bm25 groq docling\n",
    "import os\n",
    "os.environ[\"GROQ_KEY\"] = \"your_groq_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF to markdown using Docling\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = r\"C:\\Users\\madha\\OneDrive\\Documents\\PDF Data Extraction Pipeline\\Data\\s12887-020-02231-5.pdf\"\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "print(\"Document converted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Advanced chunking strategy\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_tables(markdown_text: str) -> List[Dict[str, any]]:\n",
    "    \"\"\"Extract tables from markdown and return them with their positions.\"\"\"\n",
    "    tables = []\n",
    "    # Pattern to match markdown tables\n",
    "    table_pattern = r'(\\|[^\\n]+\\|[\\n\\r]+\\|[-:\\s|]+\\|[\\n\\r]+(?:\\|[^\\n]+\\|[\\n\\r]+)*)'\n",
    "    \n",
    "    for match in re.finditer(table_pattern, markdown_text):\n",
    "        tables.append({\n",
    "            'content': match.group(0),\n",
    "            'start': match.start(),\n",
    "            'end': match.end()\n",
    "        })\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def recursive_chunk_with_overlap(text: str, max_size: int = 8000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively split text into chunks with overlap.\n",
    "    Tries to split at paragraph, then sentence, then word boundaries.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Try splitting by paragraphs first\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    if len(paragraphs) > 1:\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(current_chunk) + len(para) + 2 <= max_size:\n",
    "                current_chunk += (\"\\n\\n\" if current_chunk else \"\") + para\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    # Add overlap from end of previous chunk\n",
    "                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + para\n",
    "                else:\n",
    "                    # Single paragraph is too large, need to split further\n",
    "                    sub_chunks = recursive_chunk_with_overlap(para, max_size, overlap)\n",
    "                    chunks.extend(sub_chunks[:-1])\n",
    "                    current_chunk = sub_chunks[-1] if sub_chunks else \"\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # Try splitting by sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    if len(sentences) > 1:\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if len(current_chunk) + len(sent) + 1 <= max_size:\n",
    "                current_chunk += (\" \" if current_chunk else \"\") + sent\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \" \" + sent\n",
    "                else:\n",
    "                    # Single sentence too large, split by words\n",
    "                    sub_chunks = recursive_chunk_with_overlap(sent, max_size, overlap)\n",
    "                    chunks.extend(sub_chunks[:-1])\n",
    "                    current_chunk = sub_chunks[-1] if sub_chunks else \"\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # Last resort: split by words\n",
    "    words = text.split()\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) + 1 <= max_size:\n",
    "            current_chunk += (\" \" if current_chunk else \"\") + word\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
    "                current_chunk = overlap_text + \" \" + word\n",
    "            else:\n",
    "                # Single word exceeds max_size (rare edge case)\n",
    "                chunks.append(word[:max_size])\n",
    "                current_chunk = word[max_size:]\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def split_large_chunk(chunk_content: str, header: str, max_size: int, overlap: int = 200) -> List[Dict[str, str]]:\n",
    "    \"\"\"Split a large chunk at paragraph boundaries with overlap.\"\"\"\n",
    "    split_texts = recursive_chunk_with_overlap(chunk_content, max_size, overlap)\n",
    "    \n",
    "    split_chunks = []\n",
    "    for i, text in enumerate(split_texts, 1):\n",
    "        header_text = f\"{header} (Part {i})\" if len(split_texts) > 1 else header\n",
    "        split_chunks.append({\n",
    "            \"header\": header_text,\n",
    "            \"content\": text.strip(),\n",
    "            \"char_count\": len(text.strip()),\n",
    "            \"contains_table\": False\n",
    "        })\n",
    "    \n",
    "    return split_chunks\n",
    "\n",
    "def chunk_by_markdown_headers(markdown_text: str, \n",
    "                               min_chunk_size: int = 1000, \n",
    "                               max_chunk_size: int = 8000,\n",
    "                               overlap: int = 200) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Advanced chunking strategy:\n",
    "    1. Extract tables as separate chunks\n",
    "    2. Split by headers (## and ###) if present\n",
    "    3. If no headers, use recursive chunking with overlap\n",
    "    4. Combine small chunks (< min_chunk_size)\n",
    "    5. Split large chunks (> max_chunk_size) at paragraph boundaries with overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract tables first\n",
    "    tables = extract_tables(markdown_text)\n",
    "    table_ranges = [(t['start'], t['end']) for t in tables]\n",
    "    \n",
    "    # Split by headers\n",
    "    pattern = r'(^#{2,3}\\s+.+?)(?=\\n#{2,3}\\s+|\\Z)'\n",
    "    matches = list(re.finditer(pattern, markdown_text, re.MULTILINE | re.DOTALL))\n",
    "    \n",
    "    # If no markdown headers found, use recursive chunking\n",
    "    if not matches:\n",
    "        print(\"No markdown headers found. Using recursive chunking with overlap...\")\n",
    "        chunk_texts = recursive_chunk_with_overlap(markdown_text, max_chunk_size, overlap)\n",
    "        return [{\n",
    "            \"header\": f\"Chunk {i+1}\",\n",
    "            \"content\": text,\n",
    "            \"char_count\": len(text),\n",
    "            \"contains_table\": False\n",
    "        } for i, text in enumerate(chunk_texts)]\n",
    "    \n",
    "    raw_chunks = []\n",
    "    for match in matches:\n",
    "        chunk_text = match.group(1).strip()\n",
    "        if chunk_text and len(chunk_text) > 50:\n",
    "            header_match = re.match(r'^(#{2,3}\\s+)(.+)', chunk_text)\n",
    "            header = header_match.group(2).strip() if header_match else \"Unknown\"\n",
    "            \n",
    "            # Check if this chunk contains a table\n",
    "            chunk_start = match.start()\n",
    "            chunk_end = match.end()\n",
    "            contains_table = any(\n",
    "                (chunk_start <= t_start < chunk_end) or (chunk_start < t_end <= chunk_end)\n",
    "                for t_start, t_end in table_ranges\n",
    "            )\n",
    "            \n",
    "            raw_chunks.append({\n",
    "                \"header\": header,\n",
    "                \"content\": chunk_text,\n",
    "                \"char_count\": len(chunk_text),\n",
    "                \"contains_table\": contains_table,\n",
    "                \"position\": len(raw_chunks)\n",
    "            })\n",
    "    \n",
    "    # Process chunks: combine small, split large\n",
    "    processed_chunks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(raw_chunks):\n",
    "        current_chunk = raw_chunks[i]\n",
    "        \n",
    "        # If chunk contains a table, keep it as-is (don't combine or split)\n",
    "        if current_chunk['contains_table']:\n",
    "            processed_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # If chunk is too small, combine with next chunks\n",
    "        if current_chunk['char_count'] < min_chunk_size:\n",
    "            combined_content = current_chunk['content']\n",
    "            combined_header = current_chunk['header']\n",
    "            j = i + 1\n",
    "            \n",
    "            # Keep combining until we reach min_chunk_size or hit a table\n",
    "            while j < len(raw_chunks) and len(combined_content) < min_chunk_size:\n",
    "                if raw_chunks[j]['contains_table']:\n",
    "                    break\n",
    "                combined_content += \"\\n\\n\" + raw_chunks[j]['content']\n",
    "                combined_header += \" + \" + raw_chunks[j]['header']\n",
    "                j += 1\n",
    "            \n",
    "            # After combining, check if it's now too large and needs splitting\n",
    "            if len(combined_content) > max_chunk_size:\n",
    "                split_results = split_large_chunk(combined_content, combined_header, max_chunk_size, overlap)\n",
    "                processed_chunks.extend(split_results)\n",
    "            else:\n",
    "                processed_chunks.append({\n",
    "                    \"header\": combined_header,\n",
    "                    \"content\": combined_content,\n",
    "                    \"char_count\": len(combined_content),\n",
    "                    \"contains_table\": False\n",
    "                })\n",
    "            i = j\n",
    "            continue\n",
    "        \n",
    "        # If chunk is too large, split at paragraph boundaries\n",
    "        if current_chunk['char_count'] > max_chunk_size:\n",
    "            split_results = split_large_chunk(\n",
    "                current_chunk['content'], \n",
    "                current_chunk['header'], \n",
    "                max_chunk_size,\n",
    "                overlap\n",
    "            )\n",
    "            processed_chunks.extend(split_results)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Chunk is just right\n",
    "        processed_chunks.append(current_chunk)\n",
    "        i += 1\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "markdown_content = doc.export_to_markdown()\n",
    "chunks = chunk_by_markdown_headers(markdown_content, min_chunk_size=1000, max_chunk_size=8000, overlap=200)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk size distribution:\")\n",
    "small = sum(1 for c in chunks if c['char_count'] < 1000)\n",
    "medium = sum(1 for c in chunks if 1000 <= c['char_count'] <= 8000)\n",
    "large = sum(1 for c in chunks if c['char_count'] > 8000)\n",
    "tables = sum(1 for c in chunks if c.get('contains_table', False))\n",
    "\n",
    "print(f\"  Small (<1000): {small}\")\n",
    "print(f\"  Medium (1000-8000): {medium}\")\n",
    "print(f\"  Large (>8000): {large}\")\n",
    "print(f\"  Contains tables: {tables}\")\n",
    "\n",
    "print(f\"\\nFirst 5 chunks:\")\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    table_marker = \" [TABLE]\" if chunk.get('contains_table', False) else \"\"\n",
    "    print(f\"  {i+1}. {chunk['header'][:60]}... ({chunk['char_count']} chars){table_marker}\")\n",
    "\n",
    "if large > 0:\n",
    "    print(f\"\\nâš ï¸ WARNING: {large} chunks still exceed max size (8000 chars)\")\n",
    "    print(\"Large chunks (showing headers):\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk['char_count'] > 8000:\n",
    "            print(f\"  - {chunk['header'][:80]}... ({chunk['char_count']} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tag chunks with LLM and extract metadata\n",
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_KEY\"])\n",
    "\n",
    "tagging_prompt = \"\"\"Analyze the following text chunk from a research paper and:\n",
    "\n",
    "1. Assign relevant tags (comma-separated):\n",
    "   - <summary>: abstract, introduction, conclusion sections\n",
    "   - <research_methods>: methodology, study design, data collection\n",
    "   - <findings_conclusion>: results, findings, conclusions\n",
    "   - <metadata>: author names, publication date, affiliations\n",
    "\n",
    "2. Extract metadata if present:\n",
    "   - Authors: (list if found, else \"None\")\n",
    "   - Date: (date if found, else \"None\")\n",
    "\n",
    "Format:\n",
    "TAGS: <tag1>, <tag2>\n",
    "AUTHORS: names or None\n",
    "DATE: date or None\n",
    "\n",
    "Text:\n",
    "{chunk_text}\"\"\"\n",
    "\n",
    "document_metadata = {\"authors\": None, \"date\": None}\n",
    "tagged_chunks = []\n",
    "\n",
    "print(\"Tagging chunks with LLM...\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"  Processing {i+1}/{len(chunks)}...\", end='\\r')\n",
    "    \n",
    "    prompt = tagging_prompt.format(chunk_text=chunk['content'][:2000])\n",
    "    \n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=200,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        tags = []\n",
    "        \n",
    "        for line in result.split('\\n'):\n",
    "            if line.startswith('TAGS:'):\n",
    "                tags = [t.strip() for t in line.replace('TAGS:', '').strip().split(',')]\n",
    "            elif line.startswith('AUTHORS:'):\n",
    "                authors = line.replace('AUTHORS:', '').strip()\n",
    "                if authors.lower() != 'none' and not document_metadata[\"authors\"]:\n",
    "                    document_metadata[\"authors\"] = authors\n",
    "            elif line.startswith('DATE:'):\n",
    "                date = line.replace('DATE:', '').strip()\n",
    "                if date.lower() != 'none' and not document_metadata[\"date\"]:\n",
    "                    document_metadata[\"date\"] = date\n",
    "        \n",
    "        tagged_chunks.append({**chunk, \"tags\": tags, \"chunk_id\": i})\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  Error on chunk {i}: {e}\")\n",
    "        tagged_chunks.append({**chunk, \"tags\": [], \"chunk_id\": i})\n",
    "\n",
    "print(f\"\\n\\nMetadata extracted:\")\n",
    "print(f\"  Authors: {document_metadata['authors']}\")\n",
    "print(f\"  Date: {document_metadata['date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Store in ChromaDB with embeddings\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"research_papers\",\n",
    "    embedding_function=embedding_functions.DefaultEmbeddingFunction()\n",
    ")\n",
    "\n",
    "documents = [chunk['content'] for chunk in tagged_chunks]\n",
    "metadatas = [{\n",
    "    \"header\": chunk['header'],\n",
    "    \"tags\": ','.join(chunk['tags']),\n",
    "    \"char_count\": chunk['char_count'],\n",
    "    \"chunk_id\": chunk['chunk_id']\n",
    "} for chunk in tagged_chunks]\n",
    "ids = [f\"chunk_{chunk['chunk_id']}\" for chunk in tagged_chunks]\n",
    "\n",
    "collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "print(f\"Stored {len(documents)} chunks in ChromaDB\")\n",
    "print(f\"\\nTag distribution:\")\n",
    "tag_counts = {}\n",
    "for chunk in tagged_chunks:\n",
    "    for tag in chunk['tags']:\n",
    "        tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Implement hybrid retrieval (Semantic + BM25)\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def hybrid_search(query: str, tag_filter: str = None, top_k: int = 5, alpha: float = 0.5):\n",
    "    \"\"\"Hybrid search: semantic (alpha) + BM25 (1-alpha)\"\"\"\n",
    "    \n",
    "    # Semantic search\n",
    "    where_filter = {\"tags\": {\"$contains\": tag_filter}} if tag_filter else None\n",
    "    semantic_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k * 2,\n",
    "        where=where_filter\n",
    "    )\n",
    "    \n",
    "    # BM25 keyword search\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Normalize and combine scores\n",
    "    semantic_ids = semantic_results['ids'][0]\n",
    "    semantic_distances = semantic_results['distances'][0]\n",
    "    \n",
    "    max_dist = max(semantic_distances) if semantic_distances and max(semantic_distances) > 0 else 1\n",
    "    semantic_scores_norm = {\n",
    "        semantic_ids[i]: 1 - (semantic_distances[i] / max_dist)\n",
    "        for i in range(len(semantic_ids))\n",
    "    }\n",
    "    \n",
    "    max_bm25 = max(bm25_scores) if max(bm25_scores) > 0 else 1\n",
    "    bm25_scores_norm = bm25_scores / max_bm25\n",
    "    \n",
    "    combined_scores = {}\n",
    "    for i, chunk_id in enumerate(ids):\n",
    "        sem_score = semantic_scores_norm.get(chunk_id, 0)\n",
    "        bm25_score = bm25_scores_norm[i]\n",
    "        combined_scores[chunk_id] = alpha * sem_score + (1 - alpha) * bm25_score\n",
    "    \n",
    "    top_chunks = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for chunk_id, score in top_chunks:\n",
    "        chunk_idx = int(chunk_id.split('_')[1])\n",
    "        chunk = tagged_chunks[chunk_idx]\n",
    "        results.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"score\": score,\n",
    "            \"header\": chunk['header'],\n",
    "            \"content\": chunk['content'],\n",
    "            \"tags\": chunk['tags']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Hybrid search system ready!\")\n",
    "print(\"\\nTest query: 'research methods'\")\n",
    "test_results = hybrid_search(\"research methods\", tag_filter=\"<research_methods>\", top_k=2)\n",
    "for i, r in enumerate(test_results, 1):\n",
    "    print(f\"{i}. {r['header'][:60]} (score: {r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Test Corrective RAG with Document Extraction Query\n",
    "\n",
    "# Test the corrective RAG system\n",
    "test_query = \"What are the research methods used in this study?\"\n",
    "\n",
    "result = corrective_rag_retrieval(test_query, max_iterations=3, top_k=5)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTIVE RAG RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original Query: {result['query']}\")\n",
    "print(f\"Final Query: {result['final_query']}\")\n",
    "print(f\"Iterations: {result['iterations']}\")\n",
    "print(f\"Relevant Chunks Found: {result['total_found']}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Now use the relevant chunks to generate the answer\n",
    "if result['relevant_chunks']:\n",
    "    context = \"\\n\\n---\\n\\n\".join([chunk['content'][:2000] for chunk in result['relevant_chunks'][:3]])\n",
    "    \n",
    "    answer_prompt = f\"\"\"Based on the following relevant document sections, answer the user's question comprehensively.\n",
    "\n",
    "Question: {result['query']}\n",
    "\n",
    "Relevant Document Sections:\n",
    "{context}\n",
    "\n",
    "Provide a detailed answer based only on the information in the document sections above.\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ¤– Generating answer using LLM...\\n\")\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": answer_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_completion_tokens=1000,\n",
    "    )\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No relevant chunks found to generate an answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Complete Document Extraction using Corrective RAG\n",
    "\n",
    "def extract_document_info_with_corrective_rag():\n",
    "    \"\"\"\n",
    "    Extract all document information using corrective RAG for each section.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE DOCUMENT EXTRACTION WITH CORRECTIVE RAG\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Define extraction queries for each section\n",
    "    queries = {\n",
    "        'summary': \"What is the abstract, main purpose, and overview of this research paper?\",\n",
    "        'methods': \"What research methods, study design, data collection, and analytical approaches were used?\",\n",
    "        'findings': \"What are the key findings, results, and conclusions of this study?\"\n",
    "    }\n",
    "    \n",
    "    extraction_results = {}\n",
    "    \n",
    "    # Retrieve relevant chunks for each section using corrective RAG\n",
    "    for section, query in queries.items():\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Extracting: {section.upper()}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        result = corrective_rag_retrieval(query, max_iterations=2, top_k=5)\n",
    "        extraction_results[section] = result\n",
    "    \n",
    "    # Compile contexts\n",
    "    summary_context = \"\\n\\n---\\n\\n\".join([\n",
    "        chunk['content'][:2500] \n",
    "        for chunk in extraction_results['summary']['relevant_chunks'][:3]\n",
    "    ]) if extraction_results['summary']['relevant_chunks'] else \"No relevant information found.\"\n",
    "    \n",
    "    methods_context = \"\\n\\n---\\n\\n\".join([\n",
    "        chunk['content'][:2500] \n",
    "        for chunk in extraction_results['methods']['relevant_chunks'][:3]\n",
    "    ]) if extraction_results['methods']['relevant_chunks'] else \"No relevant information found.\"\n",
    "    \n",
    "    findings_context = \"\\n\\n---\\n\\n\".join([\n",
    "        chunk['content'][:2500] \n",
    "        for chunk in extraction_results['findings']['relevant_chunks'][:3]\n",
    "    ]) if extraction_results['findings']['relevant_chunks'] else \"No relevant information found.\"\n",
    "    \n",
    "    # Generate final extraction\n",
    "    final_prompt = f\"\"\"Based on the retrieved document sections, extract comprehensive information:\n",
    "\n",
    "## Document Information\n",
    "**Author(s):** {document_metadata['authors'] or 'Not found'}\n",
    "**Publication Date:** {document_metadata['date'] or 'Not found'}\n",
    "\n",
    "## Document Summary\n",
    "[Provide a 2-3 sentence summary of the paper's main purpose and scope]\n",
    "\n",
    "## Research Methods\n",
    "[Summarize:\n",
    "- Study design and type\n",
    "- Data sources and databases used\n",
    "- Sample size and population\n",
    "- Analytical methods and statistical approaches]\n",
    "\n",
    "## Key Findings and Conclusions\n",
    "[Summarize:\n",
    "- Primary outcomes and results\n",
    "- Statistical significance of findings\n",
    "- Main conclusions\n",
    "- Implications and recommendations]\n",
    "\n",
    "---\n",
    "\n",
    "SUMMARY CONTEXT:\n",
    "{summary_context[:3000]}\n",
    "\n",
    "---\n",
    "\n",
    "METHODS CONTEXT:\n",
    "{methods_context[:3000]}\n",
    "\n",
    "---\n",
    "\n",
    "FINDINGS CONTEXT:\n",
    "{findings_context[:3000]}\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING FINAL EXTRACTION...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        temperature=0.2,\n",
    "        max_completion_tokens=1500,\n",
    "    )\n",
    "    \n",
    "    final_extraction = response.choices[0].message.content\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL DOCUMENT EXTRACTION (CORRECTIVE RAG)\")\n",
    "    print(\"=\"*80)\n",
    "    print(final_extraction)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nExtraction Statistics:\")\n",
    "    print(f\"  Summary chunks: {len(extraction_results['summary']['relevant_chunks'])}\")\n",
    "    print(f\"  Methods chunks: {len(extraction_results['methods']['relevant_chunks'])}\")\n",
    "    print(f\"  Findings chunks: {len(extraction_results['findings']['relevant_chunks'])}\")\n",
    "    print(f\"  Total iterations: {sum(r['iterations'] for r in extraction_results.values())}\")\n",
    "    \n",
    "    return final_extraction\n",
    "\n",
    "# Run the complete extraction\n",
    "final_result = extract_document_info_with_corrective_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Agentic Corrective RAG System\n",
    "print(\"=\" * 80)\n",
    "print(\"AGENTIC CORRECTIVE RAG SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def grade_chunk_relevance(chunk_content: str, query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Grade if a chunk is relevant to the query.\n",
    "    Returns: {'relevant': bool, 'score': str, 'reason': str}\n",
    "    \"\"\"\n",
    "    grading_prompt = f\"\"\"You are a grading expert. Evaluate if the following document chunk is relevant to answer the user's query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document Chunk:\n",
    "{chunk_content[:1000]}\n",
    "\n",
    "Respond in this exact format:\n",
    "RELEVANT: yes or no\n",
    "SCORE: high, medium, or low\n",
    "REASON: brief explanation (1 sentence)\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": grading_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=150,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        relevant = False\n",
    "        score = \"low\"\n",
    "        reason = \"Unknown\"\n",
    "        \n",
    "        for line in result.split('\\n'):\n",
    "            if line.startswith('RELEVANT:'):\n",
    "                relevant = 'yes' in line.lower()\n",
    "            elif line.startswith('SCORE:'):\n",
    "                score = line.replace('SCORE:', '').strip().lower()\n",
    "            elif line.startswith('REASON:'):\n",
    "                reason = line.replace('REASON:', '').strip()\n",
    "        \n",
    "        return {\n",
    "            'relevant': relevant,\n",
    "            'score': score,\n",
    "            'reason': reason\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error grading chunk: {e}\")\n",
    "        return {'relevant': True, 'score': 'medium', 'reason': 'Error during grading'}\n",
    "\n",
    "def rewrite_query(original_query: str, feedback: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite query to improve retrieval based on feedback.\n",
    "    \"\"\"\n",
    "    if feedback:\n",
    "        rewrite_prompt = f\"\"\"The original query did not retrieve relevant results.\n",
    "\n",
    "Original Query: {original_query}\n",
    "Feedback: {feedback}\n",
    "\n",
    "Rewrite the query to be more specific and improve retrieval. Return only the rewritten query, nothing else.\"\"\"\n",
    "    else:\n",
    "        rewrite_prompt = f\"\"\"Improve the following query for better document retrieval by making it more specific and adding relevant keywords.\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Return only the improved query, nothing else.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": rewrite_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_completion_tokens=100,\n",
    "        )\n",
    "        \n",
    "        rewritten = response.choices[0].message.content.strip()\n",
    "        return rewritten\n",
    "    except Exception as e:\n",
    "        print(f\"Error rewriting query: {e}\")\n",
    "        return original_query\n",
    "\n",
    "def corrective_rag_retrieval(query: str, max_iterations: int = 3, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Agentic Corrective RAG system:\n",
    "    1. Retrieve chunks using hybrid search\n",
    "    2. Grade relevance of each chunk\n",
    "    3. If not enough relevant chunks, rewrite query and retry\n",
    "    4. Iterate until sufficient relevant chunks found or max iterations reached\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    current_query = query\n",
    "    iteration = 0\n",
    "    all_relevant_chunks = []\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"ðŸ”„ Iteration {iteration}: Searching with query: '{current_query}'\")\n",
    "        \n",
    "        # Retrieve chunks using hybrid search\n",
    "        retrieved_chunks = hybrid_search(current_query, tag_filter=None, top_k=top_k, alpha=0.5)\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            print(\"  âš ï¸  No chunks retrieved. Rewriting query...\")\n",
    "            current_query = rewrite_query(current_query, \"No results found\")\n",
    "            continue\n",
    "        \n",
    "        # Grade each chunk for relevance\n",
    "        print(f\"  ðŸ“Š Grading {len(retrieved_chunks)} chunks...\")\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for chunk in retrieved_chunks:\n",
    "            grade = grade_chunk_relevance(chunk['content'], query)\n",
    "            chunk['relevance_grade'] = grade\n",
    "            \n",
    "            if grade['relevant'] and grade['score'] in ['high', 'medium']:\n",
    "                # Avoid duplicates\n",
    "                if not any(c['chunk_id'] == chunk['chunk_id'] for c in all_relevant_chunks):\n",
    "                    all_relevant_chunks.append(chunk)\n",
    "                    relevant_count += 1\n",
    "                    print(f\"    âœ… {chunk['header'][:50]}... - {grade['score'].upper()} ({grade['reason']})\")\n",
    "            else:\n",
    "                print(f\"    âŒ {chunk['header'][:50]}... - Rejected ({grade['reason']})\")\n",
    "        \n",
    "        print(f\"  ðŸ“ˆ Found {relevant_count} relevant chunks (Total: {len(all_relevant_chunks)})\")\n",
    "        \n",
    "        # Check if we have enough relevant chunks\n",
    "        if len(all_relevant_chunks) >= 3:\n",
    "            print(f\"\\nâœ… SUCCESS: Found {len(all_relevant_chunks)} relevant chunks!\")\n",
    "            break\n",
    "        \n",
    "        # If not enough relevant chunks and we haven't hit max iterations, rewrite query\n",
    "        if iteration < max_iterations:\n",
    "            feedback = f\"Only found {len(all_relevant_chunks)} relevant chunks. Need more specific information.\"\n",
    "            print(f\"  ðŸ”„ Insufficient relevant chunks. Rewriting query...\")\n",
    "            current_query = rewrite_query(query, feedback)\n",
    "    \n",
    "    if not all_relevant_chunks:\n",
    "        print(\"\\nâš ï¸  WARNING: No relevant chunks found after all iterations.\")\n",
    "        print(\"  Falling back to top hybrid search results...\")\n",
    "        all_relevant_chunks = hybrid_search(query, top_k=top_k, alpha=0.5)\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'final_query': current_query,\n",
    "        'iterations': iteration,\n",
    "        'relevant_chunks': all_relevant_chunks,\n",
    "        'total_found': len(all_relevant_chunks)\n",
    "    }\n",
    "\n",
    "print(\"\\nCorrective RAG system ready!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
